项目设计：
	爬虫将设计为标准的模块化，主程序（通用程序）使用Cython编译为二进制文件，而相关可重定制的方面，使用python源码进行定制。可以由用户修改。

功能：
	根据一定逻辑爬取网页。
	根据一定逻辑刷新网页。
	记录网页被收录时间及确定相关的docID及唯一标识。
下载：
	下载源码
	为了减少网络带宽，提高性能，在每个线程下载同时，支持做一些parser的处理

-----------------------------------------------------------------------------------------------------
核心专题：
    分布式：
        初始化时，从一个分块进行配置，引导其他平台进行配置
        使用TCP 连接以后，进行大量的数据传输

        初始化时，进行资源的分配，尽量将各个站点分开，
        中间可以用频繁一点的交流，因为下载网页的时候，使用的相同的带宽
        定期地将各个站点做交流
        最后，重新对站点进行负载平衡，将闲置下来的服务器同时进行运行
        下载中各个站点的内容分开
-----------------------------------------------------------------------------------------------------
	核心库：
		Reptile.pyx:
			爬取网页
		urlist.pyx:
			动态运行时的url链接添加和处理,及之间内存分配(cython)
            包括运行时，分布式的一些配置
		refresher:
			同时支持对url扫描收录新网页(在已经收录一次之后，再次扫描时，判断网页是否更新及发现新的url)
		halter:
			中断器 保存断点 留作以后恢复
		communitor:
            大体框架：
                分布式下载联络器
                用于分布式计算中的判断和配置
                init:
                    初始化时候，会有一个服务器做总体的任务调度
                    做任务的调配
                runtime:
                    运行过程中，每个PC判断合home_url的链接，交给对方PC进行处理
                end:
                    本PC比较闲置时，将对对方的链接进行处理，但是数据库里面都会做标志


        htmlparser.pyx:
            基础的html解析  及基础的分词操作，配合saver进行

	IO库：
		config.py:
			配置文件
		saver:
			爬取后的网页的处理 保存
		judger:
			对爬取新url是否收录的判断

-----------------------------------------------------------------------------------------------------
数据库结构：
    
保证各个站点分开
数据库结构：
    初始化的站点url
    home_urls:
        tables:
            home_urls:
                id      site_name        url
	sites:
		tables:
			site1name
				id		url		source		psource
			site2name
				id		url		source		psource
			...
-----------------------------------------------------------------------------------------------------
!!!!!遗留问题：
    Judger中，需要考虑DNS缓存方面的问题
    Judger中，需要考虑根据DNS转换为IP的问题
	urls:
	
	框图：
		spider->judger->urlist
			  ->saver
			
		
